# Tomato - LLM Steganography

## Project Overview

Tomato is a steganography system that hides secret messages inside natural-looking text generated by an LLM. The output is statistically indistinguishable from normal LLM output, making the hidden message undetectable.

## How It Works

### Core Concept: Minimum Entropy Coupling (MEC)

MEC is a mathematical technique that couples two probability distributions (the secret message and the LLM output) such that:
1. The output text looks exactly like normal LLM output (same distribution)
2. The secret message can be recovered from the output

The key insight: an LLM generates text by sampling from a probability distribution over tokens. MEC "steers" this sampling to encode information while preserving the statistical properties.

### The Encoding Process

1. **Plaintext → Ciphertext**: XOR the secret message with a shared key
2. **Ciphertext → Stegotext**: Use FIMEC to generate LLM tokens that encode the ciphertext
3. The stegotext looks like normal LLM output but contains the hidden message

### The Decoding Process

1. **Stegotext → Ciphertext**: Use FIMEC's Bayesian inference to recover the ciphertext
2. **Ciphertext → Plaintext**: XOR with the shared key to get the original message

### Key Parameters

- `cipher_len`: Number of bytes to encode (default: 15)
- `max_len`: Maximum tokens in output (default: 200)
- `temperature`: LLM sampling temperature (default: 1.3 for better encoding)
- `k`: Top-k tokens to consider (default: 50)

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│  Encoder                                                     │
│  ├── _covertext_dist (ModelMarginal)                        │
│  │   └── lm_model (ModelWrapper or TRTLLMModelWrapper)      │
│  └── _imec (FIMEC)                                          │
│      ├── mu (RandomString - ciphertext distribution)        │
│      └── nu (ModelMarginal - covertext distribution)        │
└─────────────────────────────────────────────────────────────┘
```

## Key Files

| File | Purpose |
|------|---------|
| `tomato/encoders/encoder.py` | Main `Encoder` class with encode/decode/streaming methods |
| `tomato/utils/model_marginal.py` | `ModelMarginal` - wraps LLM for MEC compatibility |
| `tomato/utils/model_wrapper.py` | `ModelWrapper` - HuggingFace model interface |
| `tomato/utils/trtllm_model_wrapper.py` | `TRTLLMModelWrapper` - TensorRT-LLM interface |
| `server.py` | Local FastAPI server with streaming endpoints |
| `modal_server.py` | Modal deployment with H100 GPU |

## Servers

### Local Server (`server.py`)
- Uses HuggingFace models (slower, works on CPU/consumer GPU)
- Run with: `python server.py`
- Endpoints: `/encode`, `/decode`, `/encode/stream`, `/decode/stream`

### Modal Server (`modal_server.py`)
- Uses TensorRT-LLM on H100 GPU (fast)
- Deploy with: `modal deploy modal_server.py`
- Same API as local server

## Testing

```bash
# Run all unit tests (no GPU needed)
source .venv/bin/activate && python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_encoder_streaming.py -v
```

## Important Implementation Details

### ModelMarginal.with_custom_model()

Factory method to create ModelMarginal with a custom model backend:
```python
covertext_dist = ModelMarginal.with_custom_model(
    model=trtllm_wrapper,  # or any compatible model
    prompt="Write a story:",
    max_len=200,
    temperature=1.3,
    k=50
)
```

### Encoder with Custom ModelMarginal

Inject a custom ModelMarginal for different backends:
```python
encoder = Encoder(
    cipher_len=15,
    prompt="Write a story:",
    covertext_dist=covertext_dist  # your custom ModelMarginal
)
```

### Streaming

- `encoder.encode_stream()` - yields tokens as they're generated
- `encoder.decode_stream()` - yields evolving "best guess" with confidence scores

### The Shared Key

Both sender and receiver need the same key. Stored in:
- Local: `.server_key` file
- Modal: `/vol/server_key` in Modal Volume

## Dependencies

- `mec`: The MEC/FIMEC implementation (pip install from git)
- `transformers`: HuggingFace model loading
- `tensorrt-llm`: TensorRT-LLM for fast inference (Modal only)
- `fastapi`: Web server framework

## Common Issues

1. **Decode fails**: Ensure same `prompt` and `key` used for encode and decode
2. **Slow encoding**: Use Modal server with TRT-LLM for ~10x speedup
3. **Import errors**: Run from project root with venv activated
