# Modal Server Streaming API Implementation Plan

**Goal**: Add streaming encode/decode endpoints to the Modal server, achieving full API parity with `server.py`.

**Date**: 2025-11-27

---

## Table of Contents

1. [Background & Context](#1-background--context)
2. [Architecture Overview](#2-architecture-overview)
3. [Task 0: Add Factory Method to ModelMarginal](#task-0-add-factory-method-to-modelmarginal)
4. [Task 1: Modify Encoder Class](#task-1-modify-encoder-class-to-accept-custom-modelmarginal)
5. [Task 2: Update Modal Server Setup](#task-2-update-modal-server-setup)
6. [Task 3: Add Streaming Encode Endpoint](#task-3-add-streaming-encode-endpoint)
7. [Task 4: Add Streaming Decode Endpoint](#task-4-add-streaming-decode-endpoint)
8. [Task 5: Integration Testing](#task-5-integration-testing)
9. [Appendix: Key Concepts](#appendix-key-concepts)

---

## 1. Background & Context

### What is this project?

Tomato is a **steganography** system that hides secret messages inside natural-looking text generated by an LLM. It uses a technique called **Minimum Entropy Coupling (MEC)** to encode bytes into token sequences that are statistically indistinguishable from normal LLM output.

### What are we building?

We have two servers:
- **`server.py`**: Local FastAPI server using HuggingFace models (CPU/consumer GPU)
- **`modal_server.py`**: Cloud server on Modal using TensorRT-LLM on H100 GPUs (fast)

The Modal server currently has basic `/encode` and `/decode` endpoints. We need to add **streaming** versions that show tokens as they're generated in real-time.

### Why streaming?

- **Encode streaming**: Users see the stegotext appear token-by-token (like ChatGPT)
- **Decode streaming**: Users see the Bayesian inference "best guess" evolve as more tokens are processed

### Key files you'll work with

| File | Purpose |
|------|---------|
| `tomato/encoders/encoder.py` | Core `Encoder` class with encode/decode logic |
| `modal_server.py` | Modal deployment with web endpoints |
| `server.py` | Reference implementation (don't modify, just read) |
| `tomato/utils/trtllm_model_wrapper.py` | TensorRT-LLM wrapper for GPU inference |
| `tomato/utils/model_marginal.py` | Probability distribution over LLM outputs |

---

## 2. Architecture Overview

### Current flow (non-streaming)

```
Client                    Modal Server                    TRT-LLM
  |                            |                             |
  |-- POST /encode ----------->|                             |
  |                            |-- create FIMEC instance --->|
  |                            |<-- generate all tokens -----|
  |<-- JSON response ----------|                             |
```

### Target flow (streaming)

```
Client                    Modal Server                    TRT-LLM
  |                            |                             |
  |-- POST /encode/stream ---->|                             |
  |                            |-- create Encoder instance ->|
  |<-- SSE: token 1 -----------|<-- token 1 -----------------|
  |<-- SSE: token 2 -----------|<-- token 2 -----------------|
  |<-- SSE: token N -----------|<-- token N -----------------|
  |<-- SSE: complete ----------|                             |
```

### Server-Sent Events (SSE) format

SSE is a simple protocol for server-to-client streaming over HTTP:

```
data: {"type": "token", "text": "Once", "token_id": 123}\n\n
data: {"type": "token", "text": "Once upon", "token_id": 456}\n\n
data: {"type": "complete", "stegotext": [123, 456, ...]}\n\n
```

Each message is prefixed with `data: ` and ends with `\n\n`.

---

## Task 0: Add Factory Method to ModelMarginal

### Goal

Add a factory method to `ModelMarginal` that creates instances with a custom model backend. This avoids using `__new__` and manually setting attributes, which is fragile and couples to internal implementation.

### Files to modify

- `tomato/utils/model_marginal.py`

### Prerequisites

Read and understand:
- `tomato/utils/model_marginal.py` (the full class)
- `tomato/utils/trtllm_model_wrapper.py` (the TRT-LLM wrapper interface)

### Implementation

#### Step 0.1: Add the factory method

In `tomato/utils/model_marginal.py`, add this class method after the `__init__` method (around line 43):

```python
@classmethod
def with_custom_model(
    cls,
    model,
    prompt: str,
    max_len: int,
    temperature: float,
    k: int
) -> "ModelMarginal":
    """
    Create a ModelMarginal with a custom model backend.

    Use this factory method when you want to inject a custom model
    (e.g., TensorRT-LLM wrapper) instead of using the default
    HuggingFace-based ModelWrapper.

    Args:
        model: A model object with the same interface as ModelWrapper.
            Must have: vocab_size, tokenizer, top_k_conditional() method.
        prompt: Prompt to condition the generation on.
        max_len: Maximum length of the generated text.
        temperature: Temperature parameter for sampling.
        k: Number of top elements to consider during sampling.

    Returns:
        A configured ModelMarginal instance.

    Example:
        from tomato.utils.trtllm_model_wrapper import TRTLLMModelWrapper

        wrapper = TRTLLMModelWrapper(llm, tokenizer)
        dist = ModelMarginal.with_custom_model(
            model=wrapper,
            prompt="Write a story:",
            max_len=200,
            temperature=1.3,
            k=50
        )
    """
    instance = cls.__new__(cls)
    instance.max_len = max_len
    instance.temperature = temperature
    instance.k = k
    instance.branching_factor = k
    instance.lm_model = model
    instance.prompt = f"{prompt}\n"
    instance.mapping = {}
    return instance
```

### Testing

#### Unit test: Factory creates valid instance

Create `tests/test_model_marginal_factory.py`:

```python
"""Test ModelMarginal.with_custom_model factory method."""
import pytest
from unittest.mock import Mock
import numpy as np


def test_with_custom_model_creates_instance():
    """Factory should create a properly configured ModelMarginal."""
    from tomato.utils.model_marginal import ModelMarginal

    # Create a mock model with required interface
    mock_model = Mock()
    mock_model.vocab_size = 32000
    mock_model.tokenizer = Mock()
    mock_model.top_k_conditional = Mock(return_value=np.array([0.5, 0.3, 0.2]))

    # Create instance via factory
    dist = ModelMarginal.with_custom_model(
        model=mock_model,
        prompt="Test prompt",
        max_len=100,
        temperature=1.3,
        k=50
    )

    # Verify attributes are set correctly
    assert dist.lm_model is mock_model
    assert dist.prompt == "Test prompt\n"  # Note: newline added
    assert dist.max_len == 100
    assert dist.temperature == 1.3
    assert dist.k == 50
    assert dist.branching_factor == 50
    assert dist.mapping == {}


def test_with_custom_model_vs_init_parity():
    """Factory instance should behave the same as __init__ instance."""
    from tomato.utils.model_marginal import ModelMarginal

    # Create mock model
    mock_model = Mock()
    mock_model.vocab_size = 32000
    mock_model.tokenizer = Mock()

    # Create via factory
    factory_dist = ModelMarginal.with_custom_model(
        model=mock_model,
        prompt="Test",
        max_len=100,
        temperature=1.0,
        k=50
    )

    # Verify all expected attributes exist
    assert hasattr(factory_dist, 'conditional')
    assert hasattr(factory_dist, 'is_terminal')
    assert hasattr(factory_dist, 'decode')
    assert hasattr(factory_dist, 'sample')
```

Run tests:

```bash
cd /Users/jon/projects/Tomato
python -m pytest tests/test_model_marginal_factory.py -v
```

### Commit

```bash
git add tomato/utils/model_marginal.py tests/test_model_marginal_factory.py
git commit -m "feat(model_marginal): add with_custom_model factory method

Provides a clean way to create ModelMarginal instances with custom
model backends (e.g., TensorRT-LLM) without using fragile __new__
pattern.

- Add with_custom_model() classmethod
- Full docstring with example usage
- Add unit tests for factory method"
```

---

## Task 1: Modify Encoder Class to Accept Custom ModelMarginal

### Goal

Allow the `Encoder` class to use a pre-configured `ModelMarginal` instance instead of always creating its own. This lets Modal inject a TRT-LLM-backed model.

### Files to modify

- `tomato/encoders/encoder.py`

### Prerequisites

Read and understand:
- `tomato/encoders/encoder.py` lines 1-65 (constructor)
- `tomato/utils/model_marginal.py` (what ModelMarginal does)

### Implementation

#### Step 1.1: Add the new parameter

In `tomato/encoders/encoder.py`, modify the `__init__` method signature:

```python
# BEFORE (line 17-26)
def __init__(
    self,
    cipher_len: int = 15,
    shared_private_key: Optional[bytes] = None,
    prompt: str = "Good evening.",
    max_len: int = 100,
    temperature: float = 1.0,
    k: int = 50,
    model_name: str = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"
) -> None:

# AFTER
def __init__(
    self,
    cipher_len: int = 15,
    shared_private_key: Optional[bytes] = None,
    prompt: str = "Good evening.",
    max_len: int = 100,
    temperature: float = 1.0,
    k: int = 50,
    model_name: str = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    covertext_dist: Optional["ModelMarginal"] = None  # NEW
) -> None:
```

#### Step 1.2: Update the docstring

Add documentation for the new parameter in the docstring (around line 27-40):

```python
"""
...existing docstring...

    covertext_dist (ModelMarginal, optional): Pre-configured ModelMarginal instance.
        If provided, this is used instead of creating a new one. This allows
        injecting custom model backends (e.g., TensorRT-LLM). Default is None.
"""
```

#### Step 1.3: Modify the constructor body

Replace the `ModelMarginal` creation logic (around line 51-58):

```python
# BEFORE
# The covertext distribution is a distribution over innocuous content.
self._covertext_dist = ModelMarginal(
    prompt=self._prompt,
    max_len=self._max_len,
    temperature=self._temperature,
    k=self._k,
    model_name=self._model_name
)

# AFTER
# The covertext distribution is a distribution over innocuous content.
if covertext_dist is not None:
    # Use provided ModelMarginal (e.g., with TRT-LLM backend)
    self._covertext_dist = covertext_dist
else:
    # Create default ModelMarginal with HuggingFace backend
    self._covertext_dist = ModelMarginal(
        prompt=self._prompt,
        max_len=self._max_len,
        temperature=self._temperature,
        k=self._k,
        model_name=self._model_name
    )
```

#### Step 1.4: Add import for type hint

At the top of the file, add the TYPE_CHECKING import to avoid circular imports:

```python
# BEFORE (line 1-8)
import re
import secrets
from tomato.utils.random_string import RandomString
from tomato.utils.model_marginal import ModelMarginal
...

# AFTER
import re
import secrets
from typing import TYPE_CHECKING
from tomato.utils.random_string import RandomString
from tomato.utils.model_marginal import ModelMarginal
...

if TYPE_CHECKING:
    pass  # ModelMarginal already imported above, no circular issue
```

Actually, since `ModelMarginal` is already imported at the top, just use it directly in the type hint - no `TYPE_CHECKING` needed. The type hint can be:

```python
covertext_dist: Optional[ModelMarginal] = None
```

### Testing

#### Unit test: Encoder accepts custom covertext_dist

Create `tests/test_encoder_custom_dist.py`:

```python
"""Test that Encoder accepts a custom ModelMarginal instance."""
import pytest
from unittest.mock import Mock, MagicMock
import numpy as np


def test_encoder_uses_provided_covertext_dist():
    """Encoder should use provided covertext_dist instead of creating one."""
    from tomato.encoders.encoder import Encoder
    from tomato.utils.model_marginal import ModelMarginal

    # Create a mock ModelMarginal
    mock_dist = Mock(spec=ModelMarginal)
    mock_dist.max_len = 100
    mock_dist.decode = Mock(return_value="test output")

    # Create Encoder with custom dist
    encoder = Encoder(
        cipher_len=15,
        prompt="test",
        covertext_dist=mock_dist
    )

    # Verify it uses our mock, not a new instance
    assert encoder._covertext_dist is mock_dist


def test_encoder_creates_default_when_no_covertext_dist():
    """Encoder should create ModelMarginal when none provided."""
    from tomato.encoders.encoder import Encoder
    from tomato.utils.model_marginal import ModelMarginal

    # This will try to load a real model - skip in CI
    pytest.skip("Requires model download - run manually")

    encoder = Encoder(
        cipher_len=15,
        prompt="test",
        model_name="google/gemma-3-1b-it"
    )

    assert isinstance(encoder._covertext_dist, ModelMarginal)
```

Run tests:

```bash
cd /Users/jon/projects/Tomato
python -m pytest tests/test_encoder_custom_dist.py -v
```

### Commit

```bash
git add tomato/encoders/encoder.py tests/test_encoder_custom_dist.py
git commit -m "feat(encoder): accept optional covertext_dist parameter

Allows injecting a pre-configured ModelMarginal instance instead of
always creating one. This enables using custom backends like TRT-LLM.

- Add covertext_dist parameter to Encoder.__init__
- Use provided instance if given, else create default
- Add unit tests for both paths"
```

---

## Task 2: Update Modal Server Setup

### Goal

Refactor `modal_server.py` to use the `Encoder` class instead of raw FIMEC, and update timeout settings.

### Files to modify

- `modal_server.py`

### Prerequisites

Read and understand:
- `modal_server.py` (current implementation)
- `server.py` lines 148-163 (`create_encoder` function)
- `tomato/utils/model_marginal.py` (how to configure it)

### Implementation

#### Step 2.1: Update timeout constants

At the top of `modal_server.py`, update the timeout (around line 82-88):

```python
# BEFORE
@app.cls(
    image=tensorrt_image,
    gpu="H100",
    timeout=15 * 60,
    volumes={VOLUME_PATH: volume},
    container_idle_timeout=300,  # Keep warm for 5 minutes
)

# AFTER
@app.cls(
    image=tensorrt_image,
    gpu="H100",
    timeout=5 * 60,  # 5 minute request timeout
    volumes={VOLUME_PATH: volume},
    container_idle_timeout=5 * 60,  # Keep warm for 5 minutes
)
```

#### Step 2.2: Add Encoder import

Near the top of `modal_server.py`, we'll import the Encoder lazily inside methods (since it's not available at module load time on Modal). The import happens inside `setup()`.

#### Step 2.3: Refactor setup() to create TRT-LLM wrapper

Modify the `setup()` method. The existing code already creates `self.trtllm_wrapper` - keep that, but remove any ModelMarginal creation from setup (we'll create it per-request in `_create_encoder`):

```python
@modal.enter()
def setup(self):
    import os
    import torch
    from huggingface_hub import snapshot_download
    from transformers import AutoTokenizer
    from tensorrt_llm import LLM

    # ... existing key loading code (lines 100-113) - KEEP AS-IS ...

    # ... existing model download code (lines 115-123) - KEEP AS-IS ...

    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)

    # ... existing engine loading code (lines 126-140) - KEEP AS-IS ...

    # Create reusable wrapper (this line already exists)
    from tomato.utils.trtllm_model_wrapper import TRTLLMModelWrapper
    self.trtllm_wrapper = TRTLLMModelWrapper(self.llm, self.tokenizer)

    print("Server ready!")
```

Note: The existing `setup()` already creates `self.trtllm_wrapper`. Just make sure the ModelMarginal creation is removed (it will be done per-request instead).

#### Step 2.4: Add helper method to create Encoder for a request

Add a new method to create an `Encoder` instance for each request. This uses the factory method from Task 0:

```python
def _create_encoder(self, prompt: str):
    """Create an Encoder instance for a request with the given prompt."""
    from tomato.encoders.encoder import Encoder
    from tomato.utils.model_marginal import ModelMarginal

    # Create a fresh ModelMarginal with the request's prompt using factory
    covertext_dist = ModelMarginal.with_custom_model(
        model=self.trtllm_wrapper,
        prompt=prompt,
        max_len=MAX_LEN,
        temperature=DEFAULT_TEMPERATURE,
        k=DEFAULT_K
    )

    # Create Encoder with our TRT-LLM-backed ModelMarginal
    return Encoder(
        cipher_len=CIPHER_LEN,
        shared_private_key=self.server_key,
        prompt=prompt,
        max_len=MAX_LEN,
        temperature=DEFAULT_TEMPERATURE,
        k=DEFAULT_K,
        covertext_dist=covertext_dist
    )
```

**Note on constants**: `MAX_LEN`, `DEFAULT_TEMPERATURE`, `DEFAULT_K`, and `CIPHER_LEN` are already defined at the top of `modal_server.py` (lines 21-25):
```python
KEY_LENGTH = 100
CIPHER_LEN = 15
MAX_LEN = 200
DEFAULT_TEMPERATURE = 1.3
DEFAULT_K = 50
```

#### Step 2.5: Remove old _create_imec method

Delete the `_create_imec` method (lines 148-167) - it's replaced by `_create_encoder`.

#### Step 2.6: Update /encode endpoint to use Encoder

```python
@modal.web_endpoint(method="POST", docs=True)
def encode(self, plaintext: str, prompt: str) -> dict:
    """
    Encode a secret message into natural-looking text.

    The plaintext is XOR-encrypted with the server's shared key,
    then encoded into stegotext using minimum entropy coupling.
    """
    import time

    print(f"Encoding {len(plaintext)} chars...")
    start = time.perf_counter()

    encoder = self._create_encoder(prompt)
    formatted_stegotext, stegotext, _ = encoder.encode(
        plaintext,
        debug=False,
        calculate_failure_probs=False
    )

    elapsed = time.perf_counter() - start
    print(f"Encoded in {elapsed:.1f}s: {formatted_stegotext[:50]}...")

    return {
        "stegotext": [int(t) for t in stegotext],
        "formatted_stegotext": formatted_stegotext
    }
```

#### Step 2.7: Update /decode endpoint to use Encoder

```python
@modal.web_endpoint(method="POST", docs=True)
def decode(self, stegotext: list, prompt: str) -> dict:
    """
    Decode stegotext back to the original secret message.

    Requires the same prompt used during encoding.
    """
    import time

    print(f"Decoding {len(stegotext)} tokens...")
    start = time.perf_counter()

    encoder = self._create_encoder(prompt)
    estimated_plaintext, _ = encoder.decode(stegotext, debug=False)

    # Strip padding
    decoded_stripped = estimated_plaintext.rstrip("A")

    elapsed = time.perf_counter() - start
    print(f"Decoded in {elapsed:.1f}s: '{decoded_stripped}'")

    return {"plaintext": decoded_stripped}
```

### Testing

Test locally before deploying:

```bash
# Serve locally (creates temporary endpoint)
modal serve modal_server.py

# In another terminal, test the refactored endpoints:
curl -X POST "https://<your-modal-url>/encode" \
  -H "Content-Type: application/json" \
  -d '{"plaintext": "hello", "prompt": "Write a story:"}'

curl -X POST "https://<your-modal-url>/decode" \
  -H "Content-Type: application/json" \
  -d '{"stegotext": [<tokens from encode>], "prompt": "Write a story:"}'
```

### Commit

```bash
git add modal_server.py
git commit -m "refactor(modal): use Encoder class instead of raw FIMEC

- Update timeout to 5 minutes (request and idle)
- Add _create_encoder helper method
- Remove _create_imec (replaced by Encoder)
- Update /encode and /decode to use Encoder
- Prepares for streaming endpoints"
```

---

## Task 3: Add Streaming Encode Endpoint

### Goal

Add `POST /encode/stream` that returns Server-Sent Events as tokens are generated.

### Files to modify

- `modal_server.py`

### Prerequisites

Read and understand:
- `server.py` lines 404-481 (`encode_stream` endpoint)
- `tomato/encoders/encoder.py` lines 216-344 (`encode_stream` method)
- SSE format (see Appendix)

### Implementation

#### Step 3.1: Add StreamingResponse import

At the top of `modal_server.py`, add:

```python
from fastapi.responses import StreamingResponse
import json
```

#### Step 3.2: Add the streaming encode endpoint

Add this method to the `StegoServer` class:

```python
@modal.web_endpoint(method="POST", docs=True)
def encode_stream(
    self,
    plaintext: str,
    prompt: str,
    chunk_size: int = 1,
    calculate_failure_probs: bool = False
):
    """
    Encode a secret message with real-time streaming output.

    Returns Server-Sent Events (SSE) with tokens as they're generated.

    Event types:
    - 'token': A token was generated (includes accumulated text)
    - 'complete': Encoding finished (includes full stegotext)

    Args:
        plaintext: Secret message to encode
        prompt: Cover story prompt for generation
        chunk_size: Tokens per SSE event (1 = token-by-token)
        calculate_failure_probs: Include failure probability analysis
    """
    print(f"Streaming encode: {len(plaintext)} chars (chunk_size={chunk_size})")

    encoder = self._create_encoder(prompt)

    def event_generator():
        try:
            for chunk in encoder.encode_stream(
                plaintext=plaintext,
                chunk_size=chunk_size,
                calculate_failure_probs=calculate_failure_probs
            ):
                # Convert to SSE format
                yield f"data: {json.dumps(chunk)}\n\n"

            print("Streaming encode complete")

        except Exception as e:
            print(f"Streaming encode failed: {str(e)}")
            error_event = {'type': 'error', 'detail': str(e)}
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### Testing

#### Manual test with curl

```bash
# Deploy or serve locally
modal serve modal_server.py

# Test streaming (you'll see events appear one by one)
curl -N -X POST "https://<your-modal-url>/encode_stream" \
  -H "Content-Type: application/json" \
  -d '{"plaintext": "hello", "prompt": "Write a story:", "chunk_size": 1}'
```

The `-N` flag disables buffering so you see events in real-time.

Expected output:
```
data: {"type": "token", "text": "Once", "token_id": 123, "token_index": 0}

data: {"type": "token", "text": "Once upon", "token_id": 456, "token_index": 1}

...

data: {"type": "complete", "total_tokens": 45, "formatted_stegotext": "Once upon a time...", "stegotext": [123, 456, ...]}
```

#### Python test script

Create `tests/test_modal_streaming.py`:

```python
"""Integration tests for Modal streaming endpoints.

Run with: python tests/test_modal_streaming.py <modal-base-url>
"""
import sys
import json
import requests


def test_encode_stream(base_url: str):
    """Test that /encode/stream returns valid SSE events."""
    print("Testing /encode/stream...")

    response = requests.post(
        f"{base_url}/encode_stream",
        json={
            "plaintext": "test",
            "prompt": "Write a short story:",
            "chunk_size": 1
        },
        stream=True,
        headers={"Accept": "text/event-stream"}
    )

    assert response.status_code == 200, f"Expected 200, got {response.status_code}"
    assert "text/event-stream" in response.headers.get("content-type", "")

    events = []
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                event = json.loads(line[6:])
                events.append(event)
                print(f"  Event: {event.get('type')} - {str(event)[:60]}...")

    # Verify we got token events and a complete event
    token_events = [e for e in events if e.get('type') == 'token']
    complete_events = [e for e in events if e.get('type') == 'complete']

    assert len(token_events) > 0, "Expected at least one token event"
    assert len(complete_events) == 1, "Expected exactly one complete event"
    assert 'stegotext' in complete_events[0], "Complete event should have stegotext"

    print(f"  PASS: Got {len(token_events)} token events + 1 complete event")
    return complete_events[0]['stegotext']


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python test_modal_streaming.py <modal-base-url>")
        print("Example: python test_modal_streaming.py https://yourname--stego-server-stegoserver")
        sys.exit(1)

    base_url = sys.argv[1].rstrip('/')

    stegotext = test_encode_stream(base_url)
    print(f"\nAll tests passed! Stegotext has {len(stegotext)} tokens.")
```

### Commit

```bash
git add modal_server.py tests/test_modal_streaming.py
git commit -m "feat(modal): add /encode/stream SSE endpoint

Real-time streaming encode that yields tokens as FIMEC generates them.
Returns Server-Sent Events with 'token' and 'complete' event types.

- Uses Encoder.encode_stream() for streaming generation
- Supports chunk_size parameter for batching
- Supports calculate_failure_probs parameter
- Add integration test script"
```

---

## Task 4: Add Streaming Decode Endpoint

### Goal

Add `POST /decode/stream` that shows Bayesian inference progress as SSE events.

### Files to modify

- `modal_server.py`

### Prerequisites

Read and understand:
- `server.py` lines 325-401 (`decode_stream` endpoint)
- `tomato/encoders/encoder.py` lines 428-566 (`decode_stream` method)
- The decode streaming shows "best guess" evolving, not just tokens

### Implementation

#### Step 4.1: Add the streaming decode endpoint

Add this method to the `StegoServer` class:

```python
@modal.web_endpoint(method="POST", docs=True)
def decode_stream(
    self,
    stegotext: list,
    prompt: str,
    chunk_size: int = 10
):
    """
    Decode stegotext with streaming visualization of Bayesian inference.

    Returns Server-Sent Events showing the evolving "best guess" as each
    token is processed. This visualizes how confidence increases as more
    evidence accumulates.

    Event types:
    - 'token': Current best guess after processing a chunk of tokens
    - 'complete': Final decoded plaintext

    Args:
        stegotext: Token IDs from a previous encode operation
        prompt: Same prompt used during encoding
        chunk_size: Tokens to process before yielding update (default: 10)
    """
    print(f"Streaming decode: {len(stegotext)} tokens (chunk_size={chunk_size})")

    encoder = self._create_encoder(prompt)

    def event_generator():
        try:
            for chunk in encoder.decode_stream(
                stegotext=stegotext,
                chunk_size=chunk_size
            ):
                # Convert to SSE format
                yield f"data: {json.dumps(chunk)}\n\n"

            print("Streaming decode complete")

        except Exception as e:
            print(f"Streaming decode failed: {str(e)}")
            error_event = {'type': 'error', 'detail': str(e)}
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### Testing

#### Manual test with curl

```bash
# First, encode something to get stegotext
ENCODE_RESULT=$(curl -s -X POST "https://<your-modal-url>/encode" \
  -H "Content-Type: application/json" \
  -d '{"plaintext": "hello", "prompt": "Write a story:"}')

# Extract stegotext array
STEGOTEXT=$(echo $ENCODE_RESULT | jq -c '.stegotext')

# Test streaming decode
curl -N -X POST "https://<your-modal-url>/decode_stream" \
  -H "Content-Type: application/json" \
  -d "{\"stegotext\": $STEGOTEXT, \"prompt\": \"Write a story:\", \"chunk_size\": 5}"
```

Expected output:
```
data: {"type": "token", "current_guess": "h\u0000\u0000\u0000...", "tokens_processed": 5, "confidence": 0.23}

data: {"type": "token", "current_guess": "hel\u0000\u0000...", "tokens_processed": 10, "confidence": 0.45}

...

data: {"type": "complete", "plaintext": "hello", "total_tokens": 45}
```

#### Add to test script

Extend `tests/test_modal_streaming.py`:

```python
def test_decode_stream(base_url: str, stegotext: list):
    """Test that /decode/stream returns valid SSE events."""
    print("\nTesting /decode/stream...")

    response = requests.post(
        f"{base_url}/decode_stream",
        json={
            "stegotext": stegotext,
            "prompt": "Write a short story:",
            "chunk_size": 5
        },
        stream=True,
        headers={"Accept": "text/event-stream"}
    )

    assert response.status_code == 200, f"Expected 200, got {response.status_code}"

    events = []
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                event = json.loads(line[6:])
                events.append(event)
                if event.get('type') == 'token':
                    print(f"  Guess: '{event.get('current_guess', '')[:20]}...' "
                          f"(conf: {event.get('confidence', 0):.2f})")
                else:
                    print(f"  Event: {event.get('type')}")

    # Verify events
    token_events = [e for e in events if e.get('type') == 'token']
    complete_events = [e for e in events if e.get('type') == 'complete']

    assert len(token_events) > 0, "Expected at least one token event"
    assert len(complete_events) == 1, "Expected exactly one complete event"
    assert 'plaintext' in complete_events[0], "Complete event should have plaintext"

    # Verify confidence increases over time
    confidences = [e.get('confidence', 0) for e in token_events]
    if len(confidences) >= 2:
        assert confidences[-1] >= confidences[0], \
            f"Confidence should increase: {confidences[0]} -> {confidences[-1]}"

    print(f"  PASS: Decoded to '{complete_events[0]['plaintext']}'")
    return complete_events[0]['plaintext']


# Update main:
if __name__ == "__main__":
    # ... existing code ...

    stegotext = test_encode_stream(base_url)
    plaintext = test_decode_stream(base_url, stegotext)

    assert plaintext == "test", f"Roundtrip failed: expected 'test', got '{plaintext}'"
    print(f"\nAll tests passed! Roundtrip successful.")
```

### Commit

```bash
git add modal_server.py tests/test_modal_streaming.py
git commit -m "feat(modal): add /decode/stream SSE endpoint

Streaming decode showing Bayesian inference progress in real-time.
Shows evolving 'best guess' and confidence as tokens are processed.

- Uses Encoder.decode_stream() for streaming inference
- Supports chunk_size parameter for update frequency
- Confidence values show posterior convergence
- Extend integration tests with decode_stream"
```

---

## Task 5: Integration Testing

### Goal

Full end-to-end testing of all endpoints, including roundtrip verification.

### Files to create/modify

- `tests/test_modal_streaming.py` (extend)
- `tests/test_modal_full.py` (new comprehensive test)

### Implementation

#### Step 5.1: Create comprehensive test suite

Create `tests/test_modal_full.py`:

```python
"""
Comprehensive integration tests for Modal server.

Tests all endpoints including streaming, with roundtrip verification.

Usage:
    python tests/test_modal_full.py <modal-base-url>

Example:
    python tests/test_modal_full.py https://yourname--stego-server-stegoserver
"""
import sys
import json
import time
import requests
from typing import List, Dict, Any


class ModalServerTester:
    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.results: List[Dict[str, Any]] = []

    def test(self, name: str, func):
        """Run a test and record results."""
        print(f"\n{'='*60}")
        print(f"TEST: {name}")
        print('='*60)

        start = time.time()
        try:
            result = func()
            elapsed = time.time() - start
            print(f"PASS ({elapsed:.2f}s)")
            self.results.append({
                'name': name,
                'status': 'PASS',
                'time': elapsed,
                'result': result
            })
            return result
        except Exception as e:
            elapsed = time.time() - start
            print(f"FAIL ({elapsed:.2f}s): {e}")
            self.results.append({
                'name': name,
                'status': 'FAIL',
                'time': elapsed,
                'error': str(e)
            })
            return None

    def test_health(self):
        """Test /health endpoint."""
        resp = requests.get(f"{self.base_url}/health")
        assert resp.status_code == 200
        data = resp.json()
        assert data['status'] == 'healthy'
        print(f"  Model: {data.get('model')}")
        print(f"  GPU: {data.get('gpu')}")
        return data

    def test_encode(self, plaintext: str = "hello", prompt: str = "Write a story:"):
        """Test /encode endpoint."""
        resp = requests.post(
            f"{self.base_url}/encode",
            json={"plaintext": plaintext, "prompt": prompt}
        )
        assert resp.status_code == 200, f"Got {resp.status_code}: {resp.text}"
        data = resp.json()
        assert 'stegotext' in data
        assert 'formatted_stegotext' in data
        assert len(data['stegotext']) > 0
        print(f"  Tokens: {len(data['stegotext'])}")
        print(f"  Text: {data['formatted_stegotext'][:80]}...")
        return data

    def test_decode(self, stegotext: list, prompt: str = "Write a story:"):
        """Test /decode endpoint."""
        resp = requests.post(
            f"{self.base_url}/decode",
            json={"stegotext": stegotext, "prompt": prompt}
        )
        assert resp.status_code == 200, f"Got {resp.status_code}: {resp.text}"
        data = resp.json()
        assert 'plaintext' in data
        print(f"  Decoded: '{data['plaintext']}'")
        return data

    def test_encode_stream(self, plaintext: str = "test", prompt: str = "Write a story:"):
        """Test /encode/stream endpoint."""
        resp = requests.post(
            f"{self.base_url}/encode_stream",
            json={"plaintext": plaintext, "prompt": prompt, "chunk_size": 1},
            stream=True
        )
        assert resp.status_code == 200

        events = []
        for line in resp.iter_lines():
            if line and line.decode('utf-8').startswith('data: '):
                event = json.loads(line.decode('utf-8')[6:])
                events.append(event)

        token_events = [e for e in events if e.get('type') == 'token']
        complete_events = [e for e in events if e.get('type') == 'complete']

        assert len(complete_events) == 1
        print(f"  Token events: {len(token_events)}")
        print(f"  Total tokens: {complete_events[0]['total_tokens']}")
        return complete_events[0]

    def test_decode_stream(self, stegotext: list, prompt: str = "Write a story:"):
        """Test /decode/stream endpoint."""
        resp = requests.post(
            f"{self.base_url}/decode_stream",
            json={"stegotext": stegotext, "prompt": prompt, "chunk_size": 5},
            stream=True
        )
        assert resp.status_code == 200

        events = []
        for line in resp.iter_lines():
            if line and line.decode('utf-8').startswith('data: '):
                event = json.loads(line.decode('utf-8')[6:])
                events.append(event)
                if event.get('type') == 'token':
                    conf = event.get('confidence', 0)
                    guess = event.get('current_guess', '')[:15]
                    print(f"  [{event['tokens_processed']:3d}] {conf:.2f} '{guess}...'")

        complete_events = [e for e in events if e.get('type') == 'complete']
        assert len(complete_events) == 1
        print(f"  Final: '{complete_events[0]['plaintext']}'")
        return complete_events[0]

    def test_roundtrip(self, plaintext: str = "secret"):
        """Test full encode -> decode roundtrip."""
        prompt = "Write a short story about a magical forest:"

        # Encode
        encode_resp = requests.post(
            f"{self.base_url}/encode",
            json={"plaintext": plaintext, "prompt": prompt}
        )
        assert encode_resp.status_code == 200
        stegotext = encode_resp.json()['stegotext']

        # Decode
        decode_resp = requests.post(
            f"{self.base_url}/decode",
            json={"stegotext": stegotext, "prompt": prompt}
        )
        assert decode_resp.status_code == 200
        decoded = decode_resp.json()['plaintext']

        print(f"  Original:  '{plaintext}'")
        print(f"  Decoded:   '{decoded}'")
        assert decoded == plaintext, f"Roundtrip failed: '{plaintext}' -> '{decoded}'"
        return {'original': plaintext, 'decoded': decoded, 'match': True}

    def test_roundtrip_streaming(self, plaintext: str = "hi"):
        """Test full streaming encode -> streaming decode roundtrip."""
        prompt = "Tell me about space exploration:"

        # Streaming encode
        encode_resp = requests.post(
            f"{self.base_url}/encode_stream",
            json={"plaintext": plaintext, "prompt": prompt, "chunk_size": 1},
            stream=True
        )

        stegotext = None
        for line in encode_resp.iter_lines():
            if line and line.decode('utf-8').startswith('data: '):
                event = json.loads(line.decode('utf-8')[6:])
                if event.get('type') == 'complete':
                    stegotext = event['stegotext']

        assert stegotext is not None
        print(f"  Encoded {len(stegotext)} tokens")

        # Streaming decode
        decode_resp = requests.post(
            f"{self.base_url}/decode_stream",
            json={"stegotext": stegotext, "prompt": prompt, "chunk_size": 10},
            stream=True
        )

        decoded = None
        for line in decode_resp.iter_lines():
            if line and line.decode('utf-8').startswith('data: '):
                event = json.loads(line.decode('utf-8')[6:])
                if event.get('type') == 'complete':
                    decoded = event['plaintext']

        print(f"  Original:  '{plaintext}'")
        print(f"  Decoded:   '{decoded}'")
        assert decoded == plaintext, f"Streaming roundtrip failed"
        return {'original': plaintext, 'decoded': decoded, 'match': True}

    def run_all(self):
        """Run all tests."""
        print("\n" + "="*60)
        print("MODAL SERVER INTEGRATION TESTS")
        print(f"Target: {self.base_url}")
        print("="*60)

        # Basic tests
        self.test("Health Check", self.test_health)

        # Non-streaming
        encode_result = self.test(
            "Encode (non-streaming)",
            lambda: self.test_encode("hello", "Write a story:")
        )

        if encode_result:
            self.test(
                "Decode (non-streaming)",
                lambda: self.test_decode(encode_result['stegotext'], "Write a story:")
            )

        # Streaming
        stream_encode_result = self.test(
            "Encode (streaming)",
            lambda: self.test_encode_stream("test", "Write a poem:")
        )

        if stream_encode_result:
            self.test(
                "Decode (streaming)",
                lambda: self.test_decode_stream(
                    stream_encode_result['stegotext'],
                    "Write a poem:"
                )
            )

        # Roundtrips
        self.test("Roundtrip (non-streaming)", lambda: self.test_roundtrip("secret"))
        self.test("Roundtrip (streaming)", lambda: self.test_roundtrip_streaming("hi"))

        # Summary
        print("\n" + "="*60)
        print("SUMMARY")
        print("="*60)

        passed = sum(1 for r in self.results if r['status'] == 'PASS')
        failed = sum(1 for r in self.results if r['status'] == 'FAIL')

        for r in self.results:
            status = "PASS" if r['status'] == 'PASS' else "FAIL"
            print(f"  [{status}] {r['name']} ({r['time']:.2f}s)")

        print(f"\nTotal: {passed} passed, {failed} failed")
        return failed == 0


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python test_modal_full.py <modal-base-url>")
        print("Example: python test_modal_full.py https://yourname--stego-server-stegoserver")
        sys.exit(1)

    tester = ModalServerTester(sys.argv[1])
    success = tester.run_all()
    sys.exit(0 if success else 1)
```

### Running the tests

```bash
# Deploy the server
modal deploy modal_server.py

# Run full test suite
python tests/test_modal_full.py https://yourname--stego-server-stegoserver
```

### Commit

```bash
git add tests/test_modal_full.py
git commit -m "test(modal): add comprehensive integration test suite

Full test coverage for Modal server including:
- Health check
- Non-streaming encode/decode
- Streaming encode/decode
- Roundtrip verification (both modes)

Usage: python tests/test_modal_full.py <modal-url>"
```

### Final deployment

```bash
# Deploy to Modal
modal deploy modal_server.py

# Verify deployment
curl https://yourname--stego-server-stegoserver-health.modal.run

# Run full test suite
python tests/test_modal_full.py https://yourname--stego-server-stegoserver
```

### Final commit

```bash
git add -A
git commit -m "feat(modal): complete streaming API implementation

Full API parity with server.py:
- GET /health - health check
- POST /encode - non-streaming encode
- POST /decode - non-streaming decode
- POST /encode/stream - SSE streaming encode
- POST /decode/stream - SSE streaming decode

All endpoints use TensorRT-LLM on H100 GPU via the Encoder class
with injected ModelMarginal."
```

---

## Appendix: Key Concepts

### Steganography

Hiding secret messages inside innocent-looking content. In this system:
- **Plaintext**: The secret message ("Attack at dawn!")
- **Ciphertext**: XOR-encrypted plaintext (random-looking bytes)
- **Stegotext**: Natural-looking LLM output that encodes the ciphertext

### Minimum Entropy Coupling (MEC)

A mathematical technique that generates text statistically indistinguishable from normal LLM output while encoding arbitrary data. The `FIMEC` class implements this.

### Server-Sent Events (SSE)

A simple protocol for server-to-client streaming:

```
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

data: {"type": "token", "text": "Hello"}\n\n
data: {"type": "token", "text": "Hello world"}\n\n
data: {"type": "complete"}\n\n
```

- Each message starts with `data: `
- Each message ends with `\n\n` (two newlines)
- Client receives messages as they arrive (no buffering)

### TensorRT-LLM

NVIDIA's optimized inference engine for LLMs. Much faster than HuggingFace on GPUs. The `TRTLLMModelWrapper` class makes it compatible with our `ModelMarginal` interface.

### Modal

Serverless GPU platform. Key concepts:
- `@modal.cls()`: Define a class that runs on Modal infrastructure
- `@modal.enter()`: Setup code run once when container starts
- `@modal.web_endpoint()`: Expose a method as an HTTP endpoint
- `modal.Volume`: Persistent storage across container restarts
- `modal deploy`: Deploy to production
- `modal serve`: Run locally for testing

### Bayesian Inference in Decoding

When decoding, we don't know the original message. As we process each token, we update our **posterior distribution** over possible byte values. Early tokens give weak signals (low confidence), but as evidence accumulates, the posterior "peaks" around the correct values.

The streaming decode visualizes this - you can watch the confidence increase from ~0.1 to ~0.99 as more tokens are processed.

---

## Checklist

- [ ] Task 0: Add `with_custom_model` factory to ModelMarginal
- [ ] Task 0: Add unit tests for factory method
- [ ] Task 0: Commit
- [ ] Task 1: Modify Encoder class (add `covertext_dist` param)
- [ ] Task 1: Add unit tests for Encoder modification
- [ ] Task 1: Commit
- [ ] Task 2: Update Modal server timeouts
- [ ] Task 2: Add `_create_encoder` helper (using factory)
- [ ] Task 2: Refactor `/encode` and `/decode` to use Encoder
- [ ] Task 2: Test refactored endpoints
- [ ] Task 2: Commit
- [ ] Task 3: Add `/encode/stream` endpoint
- [ ] Task 3: Test with curl
- [ ] Task 3: Commit
- [ ] Task 4: Add `/decode/stream` endpoint
- [ ] Task 4: Test with curl
- [ ] Task 4: Commit
- [ ] Task 5: Create comprehensive test suite
- [ ] Task 5: Run full test suite
- [ ] Task 5: Final commit
- [ ] Deploy to Modal
- [ ] Verify all endpoints work in production
